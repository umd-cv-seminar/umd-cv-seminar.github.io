<!DOCTYPE html>
<head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap" rel="stylesheet">
    <link href="style.css" rel="stylesheet">
   
</head>
<body>
<header>
    <h1><img src="static/umd_seal_cropped.png" class="logo">UMD Computer Vision Seminar</h1>
</header>
<p>The UMD Computer Vision Seminar hosts talks from internal and external speakers on a variety of topics across computer vision, machine learning, graphics, and computational imaging.  </p>
<strong>Past: </strong>
<a href="pages/fall-2023.html">Fall 2023</a>,
<a href="pages/spring-2023.html">Spring 2023</a>
<h2>
    Spring 2024
</h2>
<p><strong>Time: </strong>Mondays 12PM-1PM</p>
<p><strong>Organizers:</strong> <a href="https://hadizayer.github.io/">Hadi Alzayer</a> and <a href="https://kevinwzhang.com">Kevin Zhang</a></p>
<p><strong>Location: </strong>IRB 4105 (sometimes 3137)</p>
<h2>
    Schedule
</h2>
<table>
    <tbody>
        <tr>
            <td class="left first"><img src="static/kevin_zhang.png" class="profile">
                <br>
                <a href="https://kevinwzhang.com">Kevin Zhang</a>
                <p class="affliation">UMD</p>
            </td>
            <td class="first"><h3>2024-02-05</h3>
                <h3 class="title">Fusing RGB and Imaging Sonar Data using Neural Surfaces</h3>
                <button type="button" class="collapsible">Click to expand abstract</button>
                <div class="content">
                <p><strong>Abstract: </strong>Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that our method dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering-based surface reconstruction methods. </p>
              </div>
            </td>
          </tr>
          
        <tr>
            <td class="left split"><img src="static/sachin_shah.jpeg" class="profile">
                <br>
                <a href="https://sachinshah.com/">Sachin Shah</a>
                <p class="affliation">UMD</p>
            </td>
            <td class="split"><h3>2024-02-05</h3>
                <h3 class="title">CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras</h3>
                <button type="button" class="collapsible">Click to expand abstract</button>
                <div class="content">
                <p><strong>Abstract: </strong>Point-spread-function (PSF) engineering is a well-established computational imaging technique used to embed extra information into images captured by RGB CMOS sensors through the customization of optical systems. However, designed optics have been largely unexplored for neuromorphic event camera sensors. This paper pushes the boundaries of computational imaging by deriving fundamental limits of event-based imaging for 3D point light source tracking. Through simulation, we illustrate that these information-theoretical bounds can design optimal lenses for event cameras. Additionally, we introduce a novel implicit neural representation system for the optimization of binary apertures, addressing the challenges associated with gradient descent in the context of binary parameters. Finally, we present a physical prototype of our learned coded aperture in conjunction with an event camera and showcase its performance for 3D tracking.</p>
                </div>
            </td>
          </tr>
      <tr>
        <td class="left split"><img src="static/aswin.jpg" class="profile">
            <br>
            <a href="http://imagesci.ece.cmu.edu/">Aswin Sankaranarayanan</a>
            <p class="affliation">CMU</p>
        </td>
        <td class="split"><h3>2024-02-12</h3>
            <h3 class="title">Computational Optics for 3D Display Design</h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
            <p><strong>Abstract: </strong>Digitization of reality is at the cusp of widespread adoption and 3D displays are at the forefront of enabling such extended reality systems. For an immersive experience,  a 3D display must faithfully reproduce the visual cues pertaining to vergence, accommodation, occlusion, and motion parallax. I will talk about recent work on computational display designs that enable such features in near-eye displays.</p>
          </div>
        </td>
      </tr>
      <tr>
        <td class="left"><img src="static/hadi_alzayer.png" class="profile">
            <br>
            <a href="https://hadizayer.github.io/">Hadi Alzayer</a>
            <p class="affliation">UMD</p>
        </td>
        <td><h3>2024-02-19</h3>
            <h3 class="title">Fixing coarse edits with diffusion models </h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
             <strong>Abstract: </strong>Editing a photograph by rearranging its components parts to produce a realistic output is a meticulous and time-consuming process, that is prone to illusion-breaking mistakes. However, making rough edits to quickly convey the intended composition and concept is easy for a human. We propose a generative method that takes a roughly edited image as input and synthesizes a photorealistic image that follows the prescribed layout, transferring fine details from the original unedited image to preserve the identity of its parts while adapting its content to the lighting and context defined by the new layout. We show that using simple segmentations and coarse 2D manipulations, we can sythesize a photorealistic edit that's faithful to the user's input, while addressing second-order effects like harmonizing the lighting and physical interactions between edited objects.
             <a href="https://arxiv.org/abs/2403.13044">Arxiv.</a>
             </div>
        </td>
      </tr>
      <tr>
        <td class="left split"><img src="static/yixuan_ren.jpg" class="profile">
            <br>
            <a href="https://www.linkedin.com/in/renyixuan/">Yixuan Ren</a>
            <p class="affliation">UMD</p>
        </td>
        <td class="split"><h3>2024-02-19</h3>
            <h3 class="title">Video Motion Customization</h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
            <strong>Abstract: </strong>Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of motion customization, we propose a one-shot method that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling from the reference videos. To disentangle the spatial and temporal information during the training pipeline, we further introduce a novel concept that detaches the original appearance from the single reference video prior to motion learning. Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion.
            <a href="https://arxiv.org/abs/2402.14780">Arxiv.</a>
          </div>
        </td>
      </tr>
      <tr>
        <td class="left split">
            <img src="static/roni_sengupta.jpg" class="profile">
            <a href="https://www.cs.unc.edu/~ronisen/">Roni Sengupta</a>
            <p>UNC Chapel Hill</p>
        </td>
        <td class="split"><h3>2024-02-26</h3>
            <h3 class="title">Building Personalized and Efficient 3D Models</h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
        <p>
            <strong>Abstract: </strong>Creating 3D models from casual camera captures enables various creative and cognitive applications for AR/VR, Computational Photography, Robotics, and Medical Imaging. Nonetheless, the underlying 'engines' propelling these 3D models comprising the hardware systems for capture and machine learning models are often expensive to build, thereby constraining widespread accessibility. In this talk, I will discuss my group's effort to develop efficient and accessible 3D models. The first part of the talk will focus on developing lightweight and personalized 3D generative models for facial reconstruction and relighting. The last part of the talk will focus on developing efficient 3D reconstruction algorithms that combine both camera and lighting variations for objects, scenes, and endoscopy images.
        </p>
      </div>
      </td>
      </tr>
      <tr>
        <td class="left">
            <img src="static/matt_walmer.jpg" class="profile">
            <a href="http://www.cs.umd.edu/~mwalmer/">Matthew Walmer</a>
            <p>UMD</p>
        </td>
        <td><h3>2024-03-11</h3>
            <h3 class="title">Teaching Matters: Investigating the Role of Supervision in Vision Transformers</h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
        <p>
            <strong>Abstract: </strong> Vision Transformers (ViTs) have gained significant popularity in recent years and have proliferated into many applications. However, it is not well explored how varied their behavior is under different learning paradigms. We compare ViTs trained through different methods of supervision, and show that they learn a diverse range of behaviors in terms of their attention, representations, and downstream performance. We also discover ViT behaviors that are consistent across supervision, including the emergence of Offset Local Attention Heads. These are self-attention heads that attend to a token adjacent to the current token with a fixed directional offset, a phenomenon that to the best of our knowledge has not been highlighted in any prior work. Our analysis shows that ViTs are highly flexible and learn to process local and global information in different orders depending on their training method. We find that contrastive self-supervised methods learn features that are competitive with explicitly supervised features, and they can even be superior for part-level tasks. We also find that the representations of reconstruction-based models show non-trivial similarity to contrastive self-supervised models. Finally, we show how the "best" layer for a given task varies by both supervision method and task, further demonstrating the differing order of information processing in ViTs.
            <a href="https://arxiv.org/abs/2212.03862">Arxiv.</a>
        </p>
        </div>
      </td>
      </tr>
      <tr>
        <td class="left split">
            <img src="static/saksham_suri.png" class="profile">
            <a href="http://www.cs.umd.edu/~sakshams/">Saksham Suri</a>
            <p>UMD</p>
        </td>
        <td class="split"><h3>2024-03-11</h3>
            <h3 class="title">Transforming ViT Features for Dense Downstream Tasks</h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
        <p>
            <strong>Abstract: </strong> We present a simple self-supervised method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a self-supervised objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. 
            <a href="https://arxiv.org/html/2403.14625v1">Arxiv.</a>
        </p>
        </div>
      </td>
      </tr>
      <tr>
        <td class="left split">
            <img src="static/yao_chih.png" class="profile">
            <a href="https://yaochih.github.io/">Yao-Chih Lee</a>
            <p>UMD</p>
        </td>
        <td class="split"><h3>2024-04-08</h3>
            <h3 class="title">Fast View Synthesis of Casual Videos</h3>
            <button type="button" class="collapsible">Click to expand abstract</button>
            <div class="content">
        <p>
            <strong>Abstract: </strong> Novel view synthesis from an in-the-wild video is difficult due to challenges like scene dynamics and lack of parallax. While existing methods have shown promising results with implicit neural radiance fields, they are slow to train and render. This paper revisits explicit video representations to synthesize high-quality novel views from a monocular video efficiently. We treat static and dynamic video content separately. Specifically, we build a global static scene model using an extended plane-based scene representation to synthesize temporally coherent novel video. Our plane-based scene representation is augmented with spherical harmonics and displacement maps to capture view-dependent effects and model non-planar complex surface geometry. We opt to represent the dynamic content as per-frame point clouds for efficiency. While such representations are inconsistency-prone, minor temporal inconsistencies are perceptually masked due to motion. We develop a method to quickly estimate such a hybrid video representation and render novel views in real time. Our experiments show that our method can render high-quality novel views from an in-the-wild video with comparable quality to state-of-the-art methods while being 100x faster in training and enabling real-time rendering.
            <a href="https://arxiv.org/abs/2312.02135">Arxiv.</a>
        </p>
        </div>
      </td>
      </tr>
    </tbody>
  </table>
<!-- <table>
     <caption>Sample Table</caption>
    <thead>
      <tr>
        <th>Date</th>
        <th>Topic</th>
        <th>Speaker</th>
        <th>Affiliation</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>2024-02-12</td>
        <td>Computational Optics for 3D Display Design</td>
        <td><a href="http://imagesci.ece.cmu.edu/"">Aswin Sankaranarayanan</a></td>
        <td>CMU</td>
      </tr>
      <tr>
        <td>2024-02-26</td>
        <td>Building Personalized and Efficient 3D Models</td>
        <td><a href="https://www.cs.unc.edu/~ronisen/">Roni Sengupta</a></td>
        <td>UNC Chapel Hill</td>
      </tr>
    </tbody>
  </table> -->

  <!-- the scripts must be at the bottom of the page -->
  <script src="scripts.js"></script>
</body>