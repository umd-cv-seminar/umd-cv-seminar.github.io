<!DOCTYPE html>
<head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap" rel="stylesheet">
    <link href="../style.css" rel="stylesheet">
</head>
<body>
<header>

    <h1><img src="../static/umd_seal_cropped.png" class="logo">UMD Computer Vision Seminar</h1>
</header>
<p>The UMD Computer Vision Seminar hosts talks from internal and external speakers on a variety of topics across computer vision, machine learning, graphics, and computational imaging.  </p>
<i>(records of internal speakers forthcoming)</i>
<h2>
    Fall 2023
</h2>
<h2>
    Schedule
</h2>
<table>
    <tbody>
        <tr>
            <td class="left">
                <img src="../static/fuxin_li.jpg", class="profile">
                <br>
                <a href="https://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>
                <p>OSU</p>
            </td>
            <td>
                <h3>2023-10-23</h3>
                <h3>From Sparse to Dense, and back to Sparse again?</h3>
                <p><strong>Abstract: </strong>Computer vision architectures used to be built on a sparse sample of points in the 80s and 90s. In the 2000s, dense models started to become popular for visual recognition as heuristically defined sparse models do not cover all the important parts of an image. However, with deep learning and end-to-end training approaches, this does not have to continue and sparse models may still have significant advantages in saving unnecessary computation as well as being more flexible. In this talk, I will talk about the deep point cloud convolutional backbones that we have developed in the past few years, including the most recent work PointConvFormer that outperforms grid-based convolutional approaches. I will also talk about a recent work, AutoFocusFormer, that uses point cloud transformer backbones and decoders to work on 2D image recognition, with a novel adaptive downsampling module that enables the end-to-end learning of adaptive downsampling. Results show significant improvements in both 3D and 2D recognition tasks. Especially, on the CityScapes benchmark, a model with only 42 million parameters with our approach outperforms the state-of-the-art Mask2Former Large model with 197 million parameters.</p>
            </td>
        </tr>
        <tr>
            <td class="left">
                <img src="../static/atlas_wang.png", class="profile">
                <br>
                <a href="https://vita-group.github.io/">Atlas Wang</a>
                <p>UT Austin</p>
            </td>
            <td>
                <h3>2023-10-26</h3>
                <h3>Attaining Sparsity in Large Language Models: Is it Easy or Hard?</h3>
                <p><strong>Abstract: </strong>In the realm of contemporary deep learning, large pre-trained transformers have seized the spotlight. Understanding the underlying frugal structures within these burgeoning models has become imperative. Although the tools of sparsity, like pruning the lottery ticket hypothesis and sparse training, have enjoyed popularity and success in traditional deep networks, their efficacy in the new era of colossal pre-trained models, such as Large Language Models (LLMs)remains uncertain. This presentation aims to elucidate two seemingly contradictory perspectives on one hand, we explore the notion that compressing LLMS is easier" compared to earlier deep models, but on the other hand, we delve into the aspects that make this endeavor "harder in its own unique way my goal is to convince you that am indeed not contradicting myself.</p>
            </td>
        </tr>
        <tr>
            <td class="left">
                <img src="../static/jun_yan_zhu.jpg", class="profile">
                <br>
                <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>
                <p>CMU</p>
            </td>
            <td>
                <h3>2023-10-27</h3>
                <h3>Enabling Collaboration between Human Creators and Generative Visual Models</h3>
                <p><strong>Abstract: </strong>Large-scale generative visual models, such as DALLE2 and Stable Diffusion, have made content creation as little effort as writing a short text description. Meanwhile, these models also spark concerns among artists, designers, and photographers about job security and proper credit for their contributions to the training images. This leads to many questions: Will generative models make creators' jobs obsolete? Should creators stop publicly sharing their work? Should we ban generative models altogether? <br>
                    In this talk, I argue that human creators and generative models can coexist. To achieve it, we need to involve creators in the loop of both model inference and model creation while crediting their efforts for their involvement.  I will first explore our recent efforts in model rewriting, which allows creators to freely control the model's behavior by adding, altering, or removing concepts and rules. I will demonstrate several applications, including creating new visual effects, customizing models with multiple personal concepts, and removing copyrighted content. I will then discuss our data attribution algorithm for assessing the influence of each training image for a generated sample. Collectively, we aim to allow creators to leverage the models while retaining control over the creation process and data ownership.</p>
            </td>
        </tr>
        <tr>
            <td class="left">
                <img src="../static/michael_zollhoefer.png", class="profile">
                <br>
                <a href="https://zollhoefer.com/">Michael Zollhoefer</a>
                <p>Meta</p>
            </td>
            <td>
                <h3>2023-12-04</h3>
                <h3>Achieving Codec Telepresence</h3>
                <p><strong>Abstract: </strong>Imagine two people living in different parts of the world. Wouldn't it be amazing if they could communicate and interact with each other as if they were co-present in the same room? Enabling such an experience virtually, i.e., building a Codec Telepresence system that is indistinguishable from reality is one of the goals of Reality Labs Research (RL-R) in Pittsburgh. To this end, we develop key technology that combines fundamental computer vision, machine learning, and graphics research based on a novel neural reconstruction and rendering paradigm. In this talk, I will explain what a Codec Telepresence system is, how it works, as well as cover recent research advances towards achieving our goal. In the future, this system will bring the world closer together by enabling anybody to communicate and interact with anyone, anywhere, at any time, as if everyone were sharing the same physical space.</p>
            </td>
        </tr>
    </tbody>
</table>
</body>

<!-- 
    <tr>
    <td class="left">
        <img src="../static/michael_zollhoefer.png", class="profile">
        <br>
        <a href="https://zollhoefer.com/">Michael Zollhoefer</a>
        <p>Meta</p>
    </td>
    <td>
        <h3>2023-12-04</h3>
        <h3></h3>
        <p><strong>Abstract: </strong></p>
    </td>
</tr>
 -->

