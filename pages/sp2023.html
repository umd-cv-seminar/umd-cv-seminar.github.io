<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap" rel="stylesheet">
    <link href="../style.css" rel="stylesheet">
    <title>UMD Computer Vision Seminar</title>
</head>
<body>
<header>
    <h1><img src="../static/umd_seal_cropped.png" class="logo">UMD Computer Vision Seminar</h1>
</header>
<p>The UMD Computer Vision Seminar hosts talks from internal and external speakers on a variety of topics across computer vision, machine learning, graphics, and computational imaging.</p>
<h2>Spring 2023</h2>
<p><strong>Time: </strong>Mondays 12PM-1PM</p>
<p><strong>Organizers:</strong> <a href="https://hadizayer.github.io/">Hadi Alzayer</a> and <a href="https://kevinwzhang.com">Kevin Zhang</a></p>
<p><strong>Location: </strong>IRB 4105 (sometimes 3137)</p>
<h2>Schedule</h2>
<!-- <table>
    <tbody> -->
        
        <table>
        <tbody> 
        <tr>
            <td colspan="2" class="date"><h3>2023-02-10</h3></td>
        </tr>
        
            <tr>
                <td class="left ">
                    <img src="../static/jiajun_wu.jpg" class="profile">
                    <a href="https://jiajunwu.com/">Jiajun Wu</a>
                    <p>Stanford</p>
                </td>
                <td class="">
                    <h3 class="title">Understanding the Visual World Through Naturally Supervised Code</h3>
                    <button type="button" class="collapsible">Click to expand abstract</button>
                    <div class="content"><p><strong>Abstract: </strong>The visual world has its inherent structure: scenes are made of multiple identical objects; different objects may have the same color or material, with a regular layout; each object can be symmetric and have repetitive parts. How can we infer, represent, and use such structure from raw data, without hampering the expressiveness of neural networks? In this talk, I will demonstrate that such structure, or code, can be learned from natural supervision. Here, natural supervision can be from pixels, where neuro-symbolic methods automatically discover repetitive parts and objects for scene synthesis. It can also be from objects, where humans during fabrication introduce priors that can be leveraged by machines to infer regular intrinsics such as texture and material. When solving these problems, structured representations and neural nets play complementary roles: it is more data-efficient to learn with structured representations, and they generalize better to new scenarios with robustly captured high-level information; neural nets effectively extract complex, low-level features from cluttered and noisy visual data.</p></div>
                </td>
            </tr>
            
        </tbody>
        </table> 
        
        <table>
        <tbody> 
        <tr>
            <td colspan="2" class="date"><h3>2023-02-27</h3></td>
        </tr>
        
            <tr>
                <td class="left ">
                    <img src="../static/anand_bhattad.jpg" class="profile">
                    <a href="https://anandbhattad.github.io/">Anand Bhattad</a>
                    <p>UIUC</p>
                </td>
                <td class="">
                    <h3 class="title">Learning about Light without Labeled Data</h3>
                    <button type="button" class="collapsible">Click to expand abstract</button>
                    <div class="content"><p><strong>Abstract: </strong>In this talk, I will show how to improve StyleGAN's image generation capabilities by incorporating simple illumination properties into the model. Our method, StyLitGAN, generates images with realistic lighting effects like shadows and reflections without any labeled, paired, or CGI data. I'll also demonstrate a near-perfect GAN inversion technique, Make It So, that outperforms previous SOTA GAN inversion methods by huge margins, able to invert and relight real scenes, even never seen out-of-domain images. Lastly, I'll show how we can have multiple scene properties predicted directly from a pretrained StyleGAN without updating or learning any new weight parameters. I will conclude by discussing their exciting implications for Generative AI.</p></div>
                </td>
            </tr>
            
        </tbody>
        </table> 
        
        <table>
        <tbody> 
        <tr>
            <td colspan="2" class="date"><h3>2023-04-10</h3></td>
        </tr>
        
            <tr>
                <td class="left ">
                    <img src="../static/angjoo_kanazawa.jpg" class="profile">
                    <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>
                    <p>UC Berkeley</p>
                </td>
                <td class="">
                    <h3 class="title">From Videos to 4D Worlds and Beyond</h3>
                    <button type="button" class="collapsible">Click to expand abstract</button>
                    <div class="content"><p><strong>Abstract: </strong>The world underlying images and videos is 3-dimensional and dynamic, with people interacting with each other, objects, and the underlying scene. Even in videos of a static scene, there is always the camera moving about in the 4D world. However, disentangling this 4D world from a video is a challenging inverse problem due to fundamental ambiguities of depth and scale. Yet, accurately recovering this information is essential for building systems that can reason about and interact with the underlying scene, and has immediate applications in visual effects and creation of immersive digital worlds. 
                    In this talk, I will discuss recent updates in 4D human perception, which includes disentangling the camera and the human motion from challenging in-the-wild videos with multiple people. Our approach takes advantage of background pixels as cues for camera motion, which when combined with motion priors and inferred ground planes can resolve scene scale and depth ambiguities up to an "anthropometric" scale. I will also talk about nerf.studio, a modular open-source framework for easily creating photorealistic 3D scenes and accelerating NeRF development. I will introduce two new works that highlight how language can be incorporated for editing and interacting with the recovered 3D scenes. These works leverage large-scale vision and language models, demonstrating the potential for multi-modal exploration and manipulation of 3D scenes.</p></div>
                </td>
            </tr>
            
        </tbody>
        </table> 
        
        <table>
        <tbody> 
        <tr>
            <td colspan="2" class="date"><h3>2023-04-11</h3></td>
        </tr>
        
            <tr>
                <td class="left ">
                    <img src="../static/ben_poole.png" class="profile">
                    <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
                    <p>Google Deepmind</p>
                </td>
                <td class="">
                    <h3 class="title">2D priors for 3D generation</h3>
                    <button type="button" class="collapsible">Click to expand abstract</button>
                    <div class="content"><p><strong>Abstract: </strong>Large scale datasets of images with text descriptions have enabled powerful models that represent and generate pixels. But progress in 3D generation has been slow due to the lack of 3D data and efficient architectures. In this talk, I'll present DreamFields and DreamFusion: two approaches that enable 3D generation from 2D priors using no 3D data. By turning 2D priors into loss functions, we can optimize 3D models (NeRFs) from scratch via gradient descent. These methods enable high-quality generation of 3D objects from diverse text prompts. Finally, I'll discuss a fundamental problem with our approach and how progress on pixel-space priors like Imagen Video and 3DiM may unlock new 3D capabilities.</p></div>
                </td>
            </tr>
            
        </tbody>
        </table> 
        
        <table>
        <tbody> 
        <tr>
            <td colspan="2" class="date"><h3>2023-05-25</h3></td>
        </tr>
        
            <tr>
                <td class="left ">
                    <img src="../static/srinath_sridhar.jpg" class="profile">
                    <a href="https://cs.brown.edu/people/ssrinath/">Srinath Sridhar</a>
                    <p>Brown University</p>
                </td>
                <td class="">
                    <h3 class="title">Foundation Models and 3D Computer Vision</h3>
                    <button type="button" class="collapsible">Click to expand abstract</button>
                    <div class="content"><p><strong>Abstract: </strong>In this partly speculative talk, I will share my thoughts on Foundation Models (aka Large Models) and their implications for object-centric 3D computer vision. To do this, I will first discuss some of our recent work on learning to generate, edit, arrange, and capture 3D objects and humans. This will include our work on (1) recursively generating and modifying 3D shapes using natural language descriptions; (2) arranging 3D shapes and re-arranging collections of shapes; and (3) capturing real-world objects and human hands. Next, using our and others' work as examples, I will speculate on how Foundation Models could provide new perspectives for addressing the same problems. I will conclude by identifying open opportunities and challenges.</p></div>
                </td>
            </tr>
            
        </tbody>
        </table> 
        
    <!-- </tbody>
</table> -->
<script src="../scripts.js"></script>
</body>
</html>
